# Generated by Django 4.2.23 on 2025-08-29 09:53

from django.conf import settings
from django.db import migrations, models
import django.db.models.deletion
import uuid


class Migration(migrations.Migration):

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ('image_processing', '0006_add_processing_progress'),
    ]

    operations = [
        migrations.CreateModel(
            name='ModelEvaluationRun',
            fields=[
                ('id', models.UUIDField(default=uuid.uuid4, editable=False, primary_key=True, serialize=False)),
                ('name', models.CharField(help_text='Human-readable name for this evaluation run', max_length=200)),
                ('description', models.TextField(blank=True, help_text='Description of what this evaluation measures')),
                ('created_at', models.DateTimeField(auto_now_add=True)),
                ('iou_threshold', models.DecimalField(decimal_places=2, default=0.5, help_text='IoU threshold for TP/FP classification', max_digits=3)),
                ('confidence_threshold', models.DecimalField(decimal_places=2, default=0.25, help_text='Minimum confidence for detections', max_digits=3)),
                ('models_evaluated', models.JSONField(default=list, help_text='List of model variants included in evaluation')),
                ('date_range_start', models.DateTimeField(blank=True, help_text='Start date for filtered data', null=True)),
                ('date_range_end', models.DateTimeField(blank=True, help_text='End date for filtered data', null=True)),
                ('species_filter', models.JSONField(default=list, help_text='Species included in evaluation')),
                ('total_images_evaluated', models.IntegerField(default=0)),
                ('total_ground_truth_objects', models.IntegerField(default=0)),
                ('total_predicted_objects', models.IntegerField(default=0)),
                ('overall_precision', models.DecimalField(blank=True, decimal_places=4, max_digits=6, null=True)),
                ('overall_recall', models.DecimalField(blank=True, decimal_places=4, max_digits=6, null=True)),
                ('overall_f1_score', models.DecimalField(blank=True, decimal_places=4, max_digits=6, null=True)),
                ('overall_map_50', models.DecimalField(blank=True, decimal_places=4, help_text='mAP@0.5', max_digits=6, null=True)),
                ('overall_map_50_95', models.DecimalField(blank=True, decimal_places=4, help_text='mAP@0.5:0.95', max_digits=6, null=True)),
                ('status', models.CharField(choices=[('PENDING', 'Pending'), ('PROCESSING', 'Processing'), ('COMPLETED', 'Completed'), ('FAILED', 'Failed')], default='PENDING', max_length=20)),
                ('error_message', models.TextField(blank=True, help_text='Error details if evaluation failed')),
                ('processing_duration', models.DurationField(blank=True, help_text='Time taken to complete evaluation', null=True)),
                ('created_by', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='evaluation_runs', to=settings.AUTH_USER_MODEL)),
            ],
            options={
                'verbose_name': 'Model Evaluation Run',
                'verbose_name_plural': 'Model Evaluation Runs',
                'ordering': ['-created_at'],
            },
        ),
        migrations.CreateModel(
            name='ModelPerformanceMetrics',
            fields=[
                ('id', models.UUIDField(default=uuid.uuid4, editable=False, primary_key=True, serialize=False)),
                ('model_name', models.CharField(help_text='Model variant name (e.g., yolov8l)', max_length=50)),
                ('model_version', models.CharField(blank=True, help_text='Specific model version or checkpoint', max_length=100)),
                ('images_processed', models.IntegerField(default=0)),
                ('ground_truth_objects', models.IntegerField(default=0, help_text="Total GT objects for this model's images")),
                ('predicted_objects', models.IntegerField(default=0, help_text='Total predictions made by this model')),
                ('true_positives', models.IntegerField(default=0)),
                ('false_positives', models.IntegerField(default=0)),
                ('false_negatives', models.IntegerField(default=0)),
                ('precision', models.DecimalField(blank=True, decimal_places=4, max_digits=6, null=True)),
                ('recall', models.DecimalField(blank=True, decimal_places=4, max_digits=6, null=True)),
                ('f1_score', models.DecimalField(blank=True, decimal_places=4, max_digits=6, null=True)),
                ('map_50', models.DecimalField(blank=True, decimal_places=4, help_text='mAP@0.5', max_digits=6, null=True)),
                ('map_50_95', models.DecimalField(blank=True, decimal_places=4, help_text='mAP@0.5:0.95', max_digits=6, null=True)),
                ('avg_inference_time_ms', models.DecimalField(blank=True, decimal_places=2, max_digits=8, null=True)),
                ('avg_confidence_score', models.DecimalField(blank=True, decimal_places=4, max_digits=5, null=True)),
                ('model_parameters_millions', models.DecimalField(blank=True, decimal_places=2, max_digits=8, null=True)),
                ('model_size_mb', models.DecimalField(blank=True, decimal_places=2, max_digits=8, null=True)),
                ('created_at', models.DateTimeField(auto_now_add=True)),
                ('evaluation_run', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='model_metrics', to='image_processing.modelevaluationrun')),
            ],
            options={
                'verbose_name': 'Model Performance Metrics',
                'verbose_name_plural': 'Model Performance Metrics',
                'ordering': ['-f1_score', '-map_50'],
                'unique_together': {('evaluation_run', 'model_name')},
            },
        ),
        migrations.AlterField(
            model_name='imageprocessingresult',
            name='total_detections',
            field=models.IntegerField(default=0),
        ),
        migrations.CreateModel(
            name='ImageEvaluationResult',
            fields=[
                ('id', models.UUIDField(default=uuid.uuid4, editable=False, primary_key=True, serialize=False)),
                ('image_filename', models.CharField(help_text='Image filename for reference', max_length=255)),
                ('model_used', models.CharField(help_text='Model variant used for this image', max_length=50)),
                ('ground_truth_boxes', models.JSONField(default=list, help_text='List of ground truth bounding boxes')),
                ('ground_truth_count', models.IntegerField(default=0)),
                ('predicted_boxes', models.JSONField(default=list, help_text='List of predicted bounding boxes')),
                ('predicted_count', models.IntegerField(default=0)),
                ('matches', models.JSONField(default=list, help_text='List of matched prediction-GT pairs with IoU scores')),
                ('unmatched_predictions', models.JSONField(default=list, help_text='FP predictions')),
                ('unmatched_ground_truth', models.JSONField(default=list, help_text='FN ground truth')),
                ('image_precision', models.DecimalField(blank=True, decimal_places=4, max_digits=6, null=True)),
                ('image_recall', models.DecimalField(blank=True, decimal_places=4, max_digits=6, null=True)),
                ('image_f1', models.DecimalField(blank=True, decimal_places=4, max_digits=6, null=True)),
                ('avg_iou', models.DecimalField(blank=True, decimal_places=4, help_text='Average IoU of matched boxes', max_digits=5, null=True)),
                ('inference_time_ms', models.DecimalField(blank=True, decimal_places=2, max_digits=8, null=True)),
                ('created_at', models.DateTimeField(auto_now_add=True)),
                ('evaluation_run', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='image_results', to='image_processing.modelevaluationrun')),
                ('image_upload', models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.CASCADE, to='image_processing.imageupload')),
            ],
            options={
                'verbose_name': 'Image Evaluation Result',
                'verbose_name_plural': 'Image Evaluation Results',
                'ordering': ['-created_at'],
            },
        ),
        migrations.CreateModel(
            name='SpeciesPerformanceMetrics',
            fields=[
                ('id', models.UUIDField(default=uuid.uuid4, editable=False, primary_key=True, serialize=False)),
                ('species_name', models.CharField(help_text='Species name (e.g., Chinese Egret)', max_length=100)),
                ('species_class_id', models.IntegerField(help_text='YOLO class ID for this species')),
                ('true_positives', models.IntegerField(default=0)),
                ('false_positives', models.IntegerField(default=0)),
                ('false_negatives', models.IntegerField(default=0)),
                ('precision', models.DecimalField(blank=True, decimal_places=4, max_digits=6, null=True)),
                ('recall', models.DecimalField(blank=True, decimal_places=4, max_digits=6, null=True)),
                ('f1_score', models.DecimalField(blank=True, decimal_places=4, max_digits=6, null=True)),
                ('average_precision', models.DecimalField(blank=True, decimal_places=4, help_text='AP@0.5 for this species', max_digits=6, null=True)),
                ('avg_confidence', models.DecimalField(blank=True, decimal_places=4, max_digits=5, null=True)),
                ('ground_truth_count', models.IntegerField(default=0, help_text='Total GT instances of this species')),
                ('detected_count', models.IntegerField(default=0, help_text='Total detections for this species')),
                ('created_at', models.DateTimeField(auto_now_add=True)),
                ('model_metrics', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='species_metrics', to='image_processing.modelperformancemetrics')),
            ],
            options={
                'verbose_name': 'Species Performance Metrics',
                'verbose_name_plural': 'Species Performance Metrics',
                'ordering': ['species_name'],
                'unique_together': {('model_metrics', 'species_name')},
            },
        ),
        migrations.CreateModel(
            name='ConfusionMatrixEntry',
            fields=[
                ('id', models.UUIDField(default=uuid.uuid4, editable=False, primary_key=True, serialize=False)),
                ('actual_species', models.CharField(help_text='Ground truth species', max_length=100)),
                ('predicted_species', models.CharField(help_text="Predicted species (or 'background' for FN)", max_length=100)),
                ('count', models.IntegerField(default=0, help_text='Number of instances with this actual/predicted combination')),
                ('percentage', models.DecimalField(blank=True, decimal_places=2, help_text='Percentage of total predictions', max_digits=5, null=True)),
                ('avg_confidence', models.DecimalField(blank=True, decimal_places=4, help_text='Average confidence for this combination', max_digits=5, null=True)),
                ('avg_iou', models.DecimalField(blank=True, decimal_places=4, help_text='Average IoU for this combination', max_digits=5, null=True)),
                ('created_at', models.DateTimeField(auto_now_add=True)),
                ('evaluation_run', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='confusion_matrix', to='image_processing.modelevaluationrun')),
            ],
            options={
                'verbose_name': 'Confusion Matrix Entry',
                'verbose_name_plural': 'Confusion Matrix Entries',
                'ordering': ['-count'],
                'unique_together': {('evaluation_run', 'actual_species', 'predicted_species')},
            },
        ),
    ]
